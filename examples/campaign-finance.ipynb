{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Campaign Finance Data\n",
    "\n",
    "Hi! My name is [Nick Crews](https://www.linkedin.com/in/nicholas-b-crews/),\n",
    "and I'm a data engineer that looks at public campaign finance data.\n",
    "\n",
    "In this post, I'll walk through how I use Ibis to explore public campaign contribution\n",
    "data from the Federal Election Commission (FEC). We'll do some loading,\n",
    "cleaning, featurizing, and visualization. There will be filtering, sorting, grouping,\n",
    "and aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Data\n",
    "\n",
    "The FEC publishes raw data as csvs to an S3 bucket [here](https://cg-519a459a-0ea3-42c2-b7bc-fa1143481f74.s3-us-gov-west-1.amazonaws.com/bulk-downloads/2018/indiv18.zip). This specific file expands to a 4.3 GiB csv file.\n",
    "\n",
    "While `ibis` could load that csv file directly, in the interest of making this example notebook quick and easy to run I've downloaded and converted the relevant csvs to parquet files, and uploaded them to google cloud (all this preprocessing was also done with `ibis`). For the interested, the preprocessing script can be found [here](../scripts/prepare_campaign_finance_data.py).\n",
    "\n",
    "We can download the parquet files to work with directly using `urllib.request.urlretrieve`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "data_dir = pathlib.Path.cwd().parent / \"data\"\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "contribs_path = data_dir / \"contributions-2018.parquet\"\n",
    "comms_path = data_dir / \"committees-2018.parquet\"\n",
    "\n",
    "if not contribs_path.exists():\n",
    "    urlretrieve(\n",
    "        \"https://storage.googleapis.com/ibis-example-notebooks-data/contributions-2018.parquet\",\n",
    "        contribs_path,\n",
    "    )\n",
    "\n",
    "if not comms_path.exists():\n",
    "    urlretrieve(\n",
    "        \"https://storage.googleapis.com/ibis-example-notebooks-data/committees-2018.parquet\",\n",
    "        comms_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "Now that we have our data, let's load it into Ibis.\n",
    "\n",
    "Since our data is stored as `parquet` files, we can do that using `ibis.read_parquet`. This takes a path to a parquet file, and returns a `Table` representing the loaded data.\n",
    "\n",
    "We'll also turn on `interactive` mode, so we can peak at the query results as we work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibis\n",
    "from ibis import _\n",
    "\n",
    "ibis.options.interactive = True\n",
    "\n",
    "contribs = ibis.read_parquet(contribs_path)\n",
    "contribs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table shows just the first few rows. To see how many rows of data we actually have, we can use the `.count()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contribs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Committees Data\n",
    "\n",
    "The contributions only list an opaque `CMTE_ID` column. We want to know which actual\n",
    "committee this is. Let's load the committees table so we can lookup from\n",
    "committee ID to committee name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms = ibis.read_parquet(comms_path)\n",
    "comms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add a the committee name to the contributions table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "together = contribs.join(comms, \"CMTE_ID\")\n",
    "together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `ENTITY_TP` column. This represents the type of entity that\n",
    "made the contribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "together.ENTITY_TP.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only care about contributions from individuals.\n",
    "\n",
    "Once we filter on this column, the contents of it are irrelevant, so let's drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = together[_.ENTITY_TP == \"IND\"].drop(\"ENTITY_TP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the `TRANSACTION_DT` column was a raw string like \"MMDDYYYY\", \n",
    "so let's convert that to a proper date type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibis.expr.types import StringValue, DateValue\n",
    "\n",
    "\n",
    "def mmddyyyy_to_date(val: StringValue) -> DateValue:\n",
    "    return val.cast(str).lpad(8, \"0\").to_timestamp(\"%m%d%Y\").date()\n",
    "\n",
    "\n",
    "cleaned = cleaned.mutate(date=mmddyyyy_to_date(_.TRANSACTION_DT)).drop(\"TRANSACTION_DT\")\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TRANSACTION_PGI` column represents the type (primary, general, etc) of election,\n",
    "and the year. But it seems to be not very consistent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.TRANSACTION_PGI.topk(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_election_type(pgi: StringValue) -> StringValue:\n",
    "    \"\"\"Use the first letter of the TRANSACTION_PGI column to determine the election type\n",
    "\n",
    "    If the first letter is not one of the known election stage, then return null.\n",
    "    \"\"\"\n",
    "    election_types = {\n",
    "        \"P\": \"primary\",\n",
    "        \"G\": \"general\",\n",
    "        \"O\": \"other\",\n",
    "        \"C\": \"convention\",\n",
    "        \"R\": \"runoff\",\n",
    "        \"S\": \"special\",\n",
    "        \"E\": \"recount\",\n",
    "    }\n",
    "    first_letter = pgi[0]\n",
    "    return first_letter.substitute(election_types, else_=ibis.NA)\n",
    "\n",
    "\n",
    "cleaned = cleaned.mutate(election_type=get_election_type(_.TRANSACTION_PGI)).drop(\n",
    "    \"TRANSACTION_PGI\"\n",
    ")\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked well! There are 0 nulls in the resulting column, so we always were\n",
    "able to determine the elction type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.election_type.topk(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 1/20 of transactions are negative. These could represent refunds, or they could be data\n",
    "entry errors. Let's simply drop them to keep it simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "above_zero = cleaned.TRANSACTION_AMT > 0\n",
    "cleaned = cleaned[above_zero]\n",
    "above_zero.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Features\n",
    "\n",
    "Now that the data is cleaned up to a usable format, let's add some features.\n",
    "\n",
    "First, it's useful to categorize donations by size, placing them into buckets\n",
    "of small, medium, large, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = [\n",
    "    10,\n",
    "    50,\n",
    "    100,\n",
    "    500,\n",
    "    1000,\n",
    "    5000,\n",
    "]\n",
    "labels = [\n",
    "    \"<10\",\n",
    "    \"10-50\",\n",
    "    \"50-100\",\n",
    "    \"100-500\",\n",
    "    \"500-1000\",\n",
    "    \"1000-5000\",\n",
    "    \"5000+\",\n",
    "]\n",
    "\n",
    "\n",
    "def bucketize(vals, edges, str_labels):\n",
    "    # Uses Ibis's .bucket() method to create a categorical column\n",
    "    int_labels = vals.bucket(edges, include_under=True, include_over=True)\n",
    "    # Map the integer labels to the string labels\n",
    "    int_to_str = {str(i): s for i, s in enumerate(str_labels)}\n",
    "    return int_labels.cast(str).substitute(int_to_str)\n",
    "\n",
    "\n",
    "featured = cleaned.mutate(amount_bucket=bucketize(_.TRANSACTION_AMT, edges, labels))\n",
    "featured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "### By donation size\n",
    "\n",
    "One thing we can look at is the donation breakdown by size:\n",
    "- Are most donations small or large?\n",
    "- Where do politicians/committees get most of their money from? Large or small donations?\n",
    "\n",
    "We also will compare performance of Ibis vs pandas during this groupby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_by(table, by):\n",
    "    return table.group_by(by).agg(\n",
    "        n_donations=_.count(),\n",
    "        total_amount=_.TRANSACTION_AMT.sum(),\n",
    "        mean_amount=_.TRANSACTION_AMT.mean(),\n",
    "        median_amount=_.TRANSACTION_AMT.approx_median(),\n",
    "    )\n",
    "\n",
    "\n",
    "def summary_by_pandas(df, by):\n",
    "    return df.groupby(by, as_index=False).agg(\n",
    "        n_donations=(\"election_type\", \"count\"),\n",
    "        total_amount=(\"TRANSACTION_AMT\", \"sum\"),\n",
    "        mean_amount=(\"TRANSACTION_AMT\", \"mean\"),\n",
    "        median_amount=(\"TRANSACTION_AMT\", \"median\"),\n",
    "    )\n",
    "\n",
    "\n",
    "# persist the input data so the following timings of the group_by are accurate.\n",
    "subset = featured[\"election_type\", \"amount_bucket\", \"TRANSACTION_AMT\"]\n",
    "subset = subset.cache()\n",
    "pandas_subset = subset.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what we are actually computing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_type_and_bucket = summary_by(subset, [\"election_type\", \"amount_bucket\"])\n",
    "by_type_and_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now let's do our timings.\n",
    "\n",
    "One interesting thing to pay attention to here is the execution time for the following\n",
    "groupby. Before, we could get away with lazy execution: because we only wanted to preview\n",
    "the first few rows, we only had to compute the first few rows, so all our previews were\n",
    "very fast.\n",
    "\n",
    "But now, as soon as we do a groupby, we have to actually go through the whole dataset\n",
    "in order to compute the aggregate per group. So this is going to be slower. BUT,\n",
    "duckdb is still quite fast. It only takes .4 seconds to groupby-agg all 20 million rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time summary_by(subset, [\"election_type\", \"amount_bucket\"]).execute();  # .execute() so we actually fetch the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the same thing in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time summary_by_pandas(pandas_subset, [\"election_type\", \"amount_bucket\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my machine it takes about 3 seconds, which is about 6 times slower than duckdb.\n",
    "\n",
    "At this scale, it again doesn't matter, but you could imagine with a dataset much larger than this, it would matter.\n",
    "\n",
    "Let's also think about memory usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_subset.memory_usage(deep=True).sum() / 1e9  # GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source dataframe is couple gigabytes, so probably during the groupby,\n",
    "the peak memory usage is going to be a bit higher than this. You could use a profiler\n",
    "such as [FIL](https://github.com/pythonspeed/filprofiler) if you wanted an exact number,\n",
    "I was too lazy to use that here.\n",
    "\n",
    "Again, this works on my laptop at this dataset size, but much larger than this and I'd\n",
    "start having problems. Duckdb on the other hand is designed around working out of core\n",
    "so it should scale to datasets into the hundreds of gigabytes, much larger than your\n",
    "computer's RAM.\n",
    "\n",
    "### Back to analysis\n",
    "\n",
    "OK, let's plot the result of that groupby.\n",
    "\n",
    "Surprise! (Or maybe not...) Most donations are small. But most of the money comes\n",
    "from donations larger than $1000.\n",
    "\n",
    "Well if that's the case, why do politicians spend so much time soliciting small\n",
    "donations? One explanation is that they can use the number of donations\n",
    "as a marketing pitch, to show how popular they are, and thus how viable of a\n",
    "candidate they are.\n",
    "\n",
    "This also might explain whose interests are being served by our politicians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "# Do some bookkeeping so the buckets are displayed smallest to largest on the charts\n",
    "bucket_col = alt.Column(\"amount_bucket:N\", sort=labels)\n",
    "\n",
    "n_by_bucket = (\n",
    "    alt.Chart(by_type_and_bucket.execute())\n",
    "    .mark_bar()\n",
    "    .encode(\n",
    "        x=bucket_col,\n",
    "        y=\"n_donations:Q\",\n",
    "        color=\"election_type:N\",\n",
    "    )\n",
    ")\n",
    "total_by_bucket = (\n",
    "    alt.Chart(by_type_and_bucket.execute())\n",
    "    .mark_bar()\n",
    "    .encode(\n",
    "        x=bucket_col,\n",
    "        y=\"total_amount:Q\",\n",
    "        color=\"election_type:N\",\n",
    "    )\n",
    ")\n",
    "n_by_bucket | total_by_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By election stage\n",
    "\n",
    "Let's look at how donations break down by election stage. Do people donate\n",
    "differently for primary elections vs general elections?\n",
    "\n",
    "Let's ignore everything but primary and general elections, since they are the\n",
    "most common, and arguably the most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb2 = by_type_and_bucket[_.election_type.isin((\"primary\", \"general\"))]\n",
    "n_donations_per_election_type = _.n_donations.sum().over(group_by=\"election_type\")\n",
    "frac = _.n_donations / n_donations_per_election_type\n",
    "gb2 = gb2.mutate(frac_n_donations_per_election_type=frac)\n",
    "gb2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like primary elections get a larger proportion of small donations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(gb2.execute()).mark_bar().encode(\n",
    "    x=\"election_type:O\",\n",
    "    y=\"frac_n_donations_per_election_type:Q\",\n",
    "    color=bucket_col,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By recipient\n",
    "\n",
    "Let's look at the top players. Who gets the most donations?\n",
    "\n",
    "Far and away it is ActBlue, which acts as a conduit for donations to Democratic\n",
    "interests.\n",
    "\n",
    "Beto O'Rourke is the top individual politician, hats off to him!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_recip = summary_by(featured, \"CMTE_NM\")\n",
    "by_recip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_recip = by_recip.order_by(ibis.desc(\"n_donations\")).head(10)\n",
    "alt.Chart(top_recip.execute()).mark_bar().encode(\n",
    "    x=alt.X(\"CMTE_NM:O\", sort=\"-y\"),\n",
    "    y=\"n_donations:Q\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Location\n",
    "\n",
    "Where are the largest donations coming from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = featured.mutate(loc=_.CITY + \", \" + _.STATE).drop(\"CITY\", \"STATE\")\n",
    "by_loc = summary_by(f2, \"loc\")\n",
    "# Drop the places with a small number of donations so we're\n",
    "# resistant to outliers for the mean\n",
    "by_loc = by_loc[_.n_donations > 1000]\n",
    "by_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_by(col):\n",
    "    top = by_loc.order_by(ibis.desc(col)).head(10)\n",
    "    return (\n",
    "        alt.Chart(top.execute())\n",
    "        .mark_bar()\n",
    "        .encode(\n",
    "            x=alt.X(\"loc:O\", sort=\"-y\"),\n",
    "            y=col,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "top_by(\"n_donations\") | top_by(\"total_amount\") | top_by(\"mean_amount\") | top_by(\n",
    "    \"median_amount\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By month\n",
    "\n",
    "When do the donations come in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_month = summary_by(featured, _.date.month().name(\"month_int\"))\n",
    "# Sorta hacky, .substritute doesn't work to change dtypes (yet?)\n",
    "# so we cast to string and then do our mapping\n",
    "month_map = {\n",
    "    \"1\": \"Jan\",\n",
    "    \"2\": \"Feb\",\n",
    "    \"3\": \"Mar\",\n",
    "    \"4\": \"Apr\",\n",
    "    \"5\": \"May\",\n",
    "    \"6\": \"Jun\",\n",
    "    \"7\": \"Jul\",\n",
    "    \"8\": \"Aug\",\n",
    "    \"9\": \"Sep\",\n",
    "    \"10\": \"Oct\",\n",
    "    \"11\": \"Nov\",\n",
    "    \"12\": \"Dec\",\n",
    "}\n",
    "by_month = by_month.mutate(month_str=_.month_int.cast(str).substitute(month_map))\n",
    "by_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_in_order = list(month_map.values())\n",
    "alt.Chart(by_month.execute()).mark_bar().encode(\n",
    "    x=alt.X(\"month_str:O\", sort=months_in_order),\n",
    "    y=\"n_donations:Q\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Thanks for following along! I hope you've learned something about Ibis, and\n",
    "maybe even about campaign finance.\n",
    "\n",
    "Ibis is a great tool for exploring data. I now find myself reaching for it\n",
    "when in the past I would have reached for pandas.\n",
    "\n",
    "Some of the highlights for me:\n",
    "\n",
    "- Fast, lazy execution, a great display format, and good type hinting/editor support for a great REPL experience.\n",
    "- Very well thought-out API and semantics (e.g. `isinstance(val, NumericValue)`?? That's beautiful!)\n",
    "- Fast and fairly complete string support, since I work with a lot of text data.\n",
    "- Extremely responsive maintainers. Sometimes I've submitted multiple feature requests and bug reports in a single day, and a PR has been merged by the next day.\n",
    "- Escape hatch to SQL. I didn't have to use that here, but if something isn't supported, you can always fall back to SQL.\n",
    "\n",
    "Check out [The Ibis Website](https://ibis-project.org/) for more information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
